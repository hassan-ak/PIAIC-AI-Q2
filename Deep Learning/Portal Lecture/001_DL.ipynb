{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Deep Learning with Python Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Introduction\n",
    "- What is machine learning\n",
    "- Fundamental concepts involved in machine learning\n",
    "- Four branches of machine learning\n",
    "- What is deep learning\n",
    "- How it works\n",
    "- What it can achieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 AI Branches\n",
    "- AI\n",
    "- Machine Learning\n",
    "- Deep Learning\n",
    "- Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Artificial Intelligence\n",
    "- Idea of AI was born when scientists started to think / program computers to do tasks only a human can do. For a long time Symbolic AI ruled the world in which we maintain a large set of rules. Symbolic AI had certain limitations in solving perception problems, like recognizing/tagging an image, translating a language to other language, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Turing Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Machine Learning\n",
    "- The frustration of crafting hard coded rules made the scientists to think what if a program can infer the rules to describe the answers / results by itself. This thought poineered the field of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Classical Programs\n",
    "- Classical program\n",
    "    - input + rule = output\n",
    "- Machine learning\n",
    "    - input + output = rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Essential Things in Machine Learning\n",
    "- ML is to learn useful representations of the input data\n",
    "    - input data points\n",
    "    - examples of expected output\n",
    "    - A way to measure how good an algorithm doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Deep Learning\n",
    "- Deep learning is assentially a subset of ML that extends ML capabilities across mutlilayered neural networks to go beyond just categorizing data. DL can actually learn, self-train, essentially from massive amount of data. With DL, it's possible to combine the unique ability of computers to process massive amounts of information quickly, with the human like ability to take in, categorize, learn and adopt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Neural Network for Handwriting Recognition\n",
    "![](./snaps/1-1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Understanding How Deep Learning Works\n",
    "![](./snaps/1-2.PNG)\n",
    "- `y = wx+b`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.12 Is this an start towards Terminator Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.13 Brief History of Machine Learning\n",
    "- Probablistic Modeling\n",
    "    - Probablistic modeling is the application of the principles of statistics to data analysis. It was one of the earliest forms of machine learning, and it still widely used to this day. One of the best known algorithms in this category is the native bayes algorithm.\n",
    "- Early neural Networks\n",
    "    - Although the core ideas of neural networks were investigated in the toy forms as early as the 1950s, the approach took decades to get started. For a long time, the missing pieces were an efficient way to train large neural networks.\n",
    "- Kernal methods\n",
    "    - Kernel methods are a group of classification algorithms, the best known of which is the support vector machine (SVM).\n",
    "    - SVMs aim at solving classification problems by finding good decision boundaries between two sets of points belonging to two different categories.\n",
    "    - A decision boundary can be thought of as a line or surface separating your training data into two spaces corresponding to two categories. To classify new data points, you just need to check which side of the decision boundary they fall on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.14 Decision Treee, Random Forest and gradient boosting machine\n",
    "![](./snaps/1-3.PNG)\n",
    "- A decison tree is a hierarchical model in whihc every node splits the samples into boundries against a rule. Every leaf of the tree is the label/prediction of the model. Before the rise of deep learning, decison tress were the most popular technique and preferred choice of ML practiners and researches. They are able to capture linear and nonlinear relations in data. Another name for decision tree in CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.15 Random Forest\n",
    "![](./snaps/1-4.PNG)\n",
    "- Random forest is a technique in whihc a large number of CARTs are used to predict the outcome and voting node is used to declare the final label on majority of votes from individual trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.16 Boosting Machines\n",
    "![](./snaps/1-5.PNG)\n",
    "- Boosting machines are algorithms in which a large number of weak learners are sequentially in such a way every learner reduces the error of its predecessor's predection.\n",
    "    - Ada Boost\n",
    "    - Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.17 Back to Deep Learning\n",
    "- What makes deep learning different\n",
    "    - it offers better performance on many problems with higher accuracy than classical techniques\n",
    "    - makes problems solving much easier by automating a very crucial and time consuming step in classical machine learning technique that is feature engineering\n",
    "- characteristics how DL learns\n",
    "    - the incremental, layer-by-layer way in which increasingly complex representations are developed\n",
    "    - these intermediate incremental representations are learned jointly\n",
    "    - each layer being updated to follow both the representational needs of the layer above and the needs of the layer below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.18 Why DL, Why Now?\n",
    "- The two key ideas of deep learning for computer vision—convolutional neural networks and backpropagation—were already well understood in 1989. The Long Short-Term Memory (LSTM) algorithm, which is fundamental to deep learning for timeseries, was developed in 1997 and has barely changed since. So why did deep learning only take off after 2012? What changed in these two decades?\n",
    "    - Hardware\n",
    "        - In past few years inroduction of GPU and vendor libraries to compute complex tasks over GPU made the deep learning shine as complex tasks on a large amount of data are solved in a considerably small time. During th elast year Google has introduced TPUs whihc are specifically designed for deep learning tasks and are even 10x faster than a GPU\n",
    "    - Datasets and benchmarks\n",
    "        - When it comes to data, in addition to the exponential progress in storage hardware over the past 20 years (following Moore’s law), the game changer has been the rise of the internet, making it feasible to collect and distribute very large datasets for machine learning. Today, large companies work with image datasets, video datasets, and natural-language datasets that couldn’t have been collected without the internet. User-generated image tags on Flickr, for instance, have been a treasure trove of data for computer vision. So are YouTube videos. And Wikipedia is a key dataset for natural-language processing.\n",
    "    - Algorithmic advances\n",
    "        - In addition to hardware and data, until the late 2000s, we were missing a reliable way to train very deep neural networks. As a result, neural networks were still fairly shallow, using only one or two layers of representations; thus, they weren’t able to shine against more-refined shallow methods such as SVMs and random forests. The key issue was that of gradient propagation through deep stacks of layers. The feedback signal used to train neural networks would fade away as the number of layers increased. This changed around 2009–2010 with the advent of several simpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.19 A New Wave of Investment\n",
    "- AI and machine learning have the potential to create an additional $2.6T in value by 2020 in marketing and sales, up to $2T in manufacuring and supply chain planning\n",
    "- Gartner predicts the business value created by AI will reach $3.9T in 2022.\n",
    "- IDC predicts worldwide spending on cognitive and AI systems will reach $77.6B in 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.20 The Democratization of Deep learning\n",
    "- Introduction of new tools for languages that support Deep Learning made it ti approachable to a common developer with the knowledge of high level scripting language like python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.21 Will It Last\n",
    "- Deep learning has several properties that justify its status as an AI revolution, and it’s here to stay. We may not be using neural networks two decades from now, but whatever we use will directly inherit from modern deep learning and its core concepts.\n",
    "    - Simplicity\n",
    "        - Deep learning removes the need for feature engineering, replacing complex, brittle, engineering-heavy pipelines with simple, end-to-end trainable models that are typically built using only five or six different tensor operations.\n",
    "    - Scalability\n",
    "        - Deep learning is highly amenable to parallelization on GPUs or TPUs, so it can take full advantage of Moore’s law. In addition, deep-learning models are trained by iterating over small batches of data, allowing them to be trained on datasets of arbitrary size. (The only bottleneck is the amount of parallel computational power available, which, thanks to Moore’s law, is a fastmoving barrier.)\n",
    "    - Versatility and reusability\n",
    "        - Unlike many prior machine-learning approaches, deep-learning models can be trained on additional data without restarting from scratch, making them viable for continuous online learning—an important property for very large production models. Furthermore, trained deep-learning models are repurposable and thus reusable: for instance, it’s possible to take a deep-learning model trained for image classification and drop it into a video processing pipeline. This allows us to reinvest previous work into increasingly complex and powerful models. This also makes deep learning applicable to fairly small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.22 Summary"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
