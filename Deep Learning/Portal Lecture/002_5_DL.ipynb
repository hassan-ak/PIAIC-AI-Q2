{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Data Representations for Neural Network\n",
    "- Basic building blocks for data passing into neural networks are numpy arrays, also called tensors. At its core, a tensor is a container for data—almost always numerical data. So, it’s a container for numbers. You may be already familiar with matrices, which are 2D tensors: tensors are a generalization of matrices to an arbitrary number of dimensions (note that in the context of tensors, a dimension is often called an axis). The number of axes of a tensor is also called its rank.\n",
    "- Scalars (0D Tensors)\n",
    "    - A tensor that contains only one number is called a scalar (or scalar tensor, or 0-dimensional tensor, or 0D tensor).\n",
    "        ```.py\n",
    "        import numpy as np\n",
    "        x = np.array(12)\n",
    "        x      --> array(12)\n",
    "        x.ndim --> 0\n",
    "        ```\n",
    "- Vectors (1D tensors)\n",
    "    - An array of numbers is called a vector, or 1D tensor. A 1D tensor is said to have exactly one axis.\n",
    "        ```\n",
    "        x = np.array([12, 3, 6, 14])\n",
    "        x       --> array([12, 3, 6, 14])\n",
    "        x.ndim  --> 1\n",
    "        ```\n",
    "- Matrices (2D tensors)\n",
    "    - An array of vectors is a matrix, or 2D tensor. A matrix has two axes (often referred to rows and columns). You can visually interpret a matrix as a rectangular grid of numbers.\n",
    "        ```\n",
    "        x = np.array([[5, 78, 2, 34, 0],\n",
    "                    [6, 79, 3, 35, 1],\n",
    "                    [7, 80, 4, 36, 2]])\n",
    "        x.ndim  --> 2\n",
    "        ```\n",
    "- 3D tensors and higher-dimensional tensors\n",
    "    - If you pack such matrices in a new array, you obtain a 3D tensor, which you can visually interpret as a cube of numbers. \n",
    "        ```\n",
    "        x = np.array([[[5, 78, 2, 34, 0],\n",
    "                    [6, 79, 3, 35, 1],\n",
    "                    [7, 80, 4, 36, 2]],\n",
    "                    [[5, 78, 2, 34, 0],\n",
    "                    [6, 79, 3, 35, 1],\n",
    "                    [7, 80, 4, 36, 2]],\n",
    "                    [[5, 78, 2, 34, 0],\n",
    "                    [6, 79, 3, 35, 1],\n",
    "                    [7, 80, 4, 36, 2]]])\n",
    "        x.ndim  --> 3\n",
    "        ```\n",
    "- Key attributes of a tensor\n",
    "    - Number of axes\n",
    "    - Shape\n",
    "    - Data Type\n",
    "- Number of axes (Rank)\n",
    "    - For instance, a 3D tensor has three axes, and a matrix has two axes. This is also called the tensor’s ndim in Python libraries such as Numpy\n",
    "- Shape\n",
    "    - This is a tuple of integers that describes how many dimensions the tensor has along each axis.\n",
    "-  Data type\n",
    "    - This is the type of the data contained in the tensor; for instance, a tensor’s type could be float32, uint8, float64, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Manipulating Tensors with Numpy\n",
    "- All the numpy operations can be performed on tensors\n",
    "- The notion of data batches\n",
    "    - deep-learning models don’t process an entire dataset at once; rather, they break the data into small batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12 The Gears of Neural Network\n",
    "- Much as any computer program can be ultimately reduced to a small set of binary operations on binary inputs (AND, OR, NOR, and so on), all transformations learned by deep neural networks can be reduced to a handful of tensor operations applied to tensors of numeric data. For instance, it’s possible to add tensors, multiply tensors, and so on.\n",
    "- Element wise operations\n",
    "    - operations that are applied independently to each entry in the tensors being considered\n",
    "-  Broadcasting\n",
    "    - What happens with addition when the shapes of the two tensors being added differ?\n",
    "    - When possible, and if there’s no ambiguity, the smaller tensor will be broadcasted to match the shape of the larger tensor. Broadcasting consists of two steps:\n",
    "        1. Axes (called broadcast axes) are added to the smaller tensor to match the ndim of the larger tensor.\n",
    "        2. The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor\n",
    "- Tensor dot\n",
    "    - The dot operation, also called a tensor product (not to be confused with an elementwise product) is the most common, most useful tensor operation. Contrary to element-wise operations, it combines entries in the input tensors.\n",
    "- Tensor reshaping\n",
    "    - A third type of tensor operation that’s essential to understand is tensor reshaping. Reshaping a tensor means rearranging its rows and columns to match a target shape. Naturally, the reshaped tensor has the same total number of coefficients as the initial tensor. \n",
    "    ```\n",
    "     x = np.array([[0., 1.],\n",
    "                   [2., 3.],\n",
    "                   [4., 5.]])\n",
    "    print(x.shape)  --> (3, 2)\n",
    "    x = x.reshape((6, 1))\n",
    "    x               --> array([[ 0.],\n",
    "                               [ 1.],\n",
    "                               [ 2.],\n",
    "                               [ 3.],\n",
    "                               [ 4.],\n",
    "                               [ 5.]])\n",
    "    x = x.reshape((2, 3))\n",
    "    x               --> array([[ 0., 1., 2.],\n",
    "                              [ 3., 4., 5.]])\n",
    "    ```\n",
    "- Geometric interpretation of tensor operations\n",
    "    - Because the contents of the tensors manipulated by tensor operations can be interpreted as coordinates of points in some geometric space, all tensor operations have a geometric interpretation.\n",
    "- A geometric interpretation of deep learning\n",
    "    - You just learned that neural networks consist entirely of chains of tensor operations and that all of these tensor operations are just geometric transformations of the input data. It follows that you can interpret a neural network as a very complex geometric transformation in a high-dimensional space, implemented via a long series of simple steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13 Gradient Base Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear regression\n",
    "\n",
    "    ![](./snaps/2-1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradient descent optimizer\n",
    "    - This happens within what’s called a training loop, which works as follows. Repeat these steps in a loop, as long as necessary:\n",
    "        1. Draw a batch of training samples x and corresponding targets y.\n",
    "        2. Run the network on x (a step called the forward pass) to obtain predictions y_pred.\n",
    "        3. Compute the loss of the network on the batch, a measure of the mismatch between y_pred and y.\n",
    "        4. Update all weights of the network in a way that slightly reduces the loss on this batch\n",
    "- What’s a derivative?\n",
    "    - The slope a is called the derivative of f in p. If a is negative, it means a small change of x around p will result in a decrease of f(x) (as shown in figure 2.10); and if a is positive, a small change in x will result in an increase of f(x).\n",
    "- Stochastic gradient descent\n",
    "    - Given a differentiable function, it’s theoretically possible to find its minimum analytically: it’s known that a function’s minimum is a point where the derivative is 0, so all you have to do is find all the points where the derivative goes to 0 and check for which of these points the function has the lowest value. Applied to a neural network, that means finding analytically the combination of weight values that yields the smallest possible loss function. This can be done by solving the equation gradient(f)(W) = 0 for W. This is a polynomial equation of N variables, where N is the number of coefficients in the network. Although it would be possible to solve such an equation for N = 2 or N = 3, doing so is intractable for real neural networks, where the number of parameters is never less than a few thousand and can often be several tens of millions.\n",
    "    - nstead, you can use the four-step algorithm outlined at the beginning of this section: modify the parameters little by little based on the current loss value on a random batch of data. Because you’re dealing with a differentiable function, you can compute its gradient, which gives you an efficient way to implement step 4. If you update the weights in the opposite direction from the gradient, the loss will be a little less every time\n",
    "- Mini-batch Stochastic gradient descent\n",
    "    1. Draw a batch of training samples x and corresponding targets y.\n",
    "    2. Run the network on x to obtain predictions y_pred.\n",
    "    3. Compute the loss of the network on the batch, a measure of the mismatch between y_pred and y.\n",
    "    4. Compute the gradient of the loss with regard to the network’s parameters (abackward pass).\n",
    "    5. Move the parameters a little in the opposite direction from the gradient—for example W -= step * gradient—thus reducing the loss on the batch a bit.\n",
    "- Chaining derivatives: the Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.14 Summary"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
