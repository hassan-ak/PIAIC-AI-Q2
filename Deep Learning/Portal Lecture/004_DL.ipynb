{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndHZlSAPrRNC"
      },
      "source": [
        "# Deep Learning 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiR2RRYErcBo"
      },
      "source": [
        "## 4.1 Machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_I7rHDssqiS"
      },
      "source": [
        "## 4.2 Supervise Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBd2EPU7tJNm"
      },
      "source": [
        "- Supervised learning\n",
        "  - map input data to known targets/annotations\n",
        "  - Categories\n",
        "    - classification\n",
        "    - regression\n",
        "    - Sequence generation\n",
        "      - Given a picture, predict a caption describing it. Sequence generation can sometimes be reformulated as a series of classification problems (such as repeatedly predicting a word or token in a sequence).\n",
        "    - Syntax tree prediction\n",
        "      - Given a sentence, predict its decomposition into a syntax tree\n",
        "    - Object detection\n",
        "      - Given a picture, draw a bounding box around certain objects inside the picture. This can also be expressed as a classification problem (given many candidate bounding boxes, classify the contents of each one) or as a joint classification and regression problem, where the bounding-box coordinates are predicted via vector regression.\n",
        "    - Image segmentation\n",
        "      - Given a picture, draw a pixel-level mask on a specific object."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoXC0vVmuU6L"
      },
      "source": [
        "## 4.3 Unsupervise Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gitC3Gt5ubTy"
      },
      "source": [
        "- Unsupervised learning\n",
        "  - finding interesting transformations of the input data without the help of any targets\n",
        "  - purposes \n",
        "    - data visualization\n",
        "    - data compression\n",
        "    - data denoising\n",
        "    - to better understand the correlations present in the data at hand.\n",
        "  - Categories\n",
        "    - Dimensionality reduction\n",
        "    - clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGuGMvqWvZxo"
      },
      "source": [
        "## 4.4 Self Supervise Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILWrakRyve1p"
      },
      "source": [
        "- Self-supervised learning / temporally supervised learning\n",
        "  - supervised learning without human-annotated labels\n",
        "  - supervision comes from future input data\n",
        "  - labels generated from the input data (heuristic algorithm)\n",
        "  - Examples\n",
        "    - autoencoders\n",
        "      - generated targets are the input, unmodified\n",
        "    - predict the next frame in a video, given past frames\n",
        "    - predict the the next word in a text, given previous words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfqc5GYexKgW"
      },
      "source": [
        "## 4.5 Reinforcement Learning\n",
        "- an agent receives information about its environment and learns to choose actions that will maximize some reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QhVy761xhr-"
      },
      "source": [
        "## 4.6 Glossory Classification & Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9pWwfe6xkzL"
      },
      "source": [
        "- Classification and regression glossary\n",
        "  - Sample or input\n",
        "    - One data point that goes into your model\n",
        "  - Prediction or output\n",
        "    - What comes out of your model.\n",
        "  - Target\n",
        "    - The truth. What your model should ideally have predicted, according to an external source of data.\n",
        "  - Prediction error or loss value\n",
        "    - A measure of the distance between your model’s prediction and the target.\n",
        "  - Classes\n",
        "    - A set of possible labels to choose from in a classification problem.\n",
        "  - Label\n",
        "    - A specific instance of a class annotation in a classification problem.\n",
        "  - Ground-truth or annotations\n",
        "    - All targets for a dataset, typically collected by humans.\n",
        "  - Binary classification\n",
        "    - A classification task where each input sample should be categorized into two exclusive categories.\n",
        "  - Multiclass classification\n",
        "    - A classification task where each input sample should be categorized into more than two categories\n",
        "  - Multilabel classification\n",
        "    - A classification task where each input sample can be assigned multiple labels.\n",
        "  - Scalar regression\n",
        "    - A task where the target is a continuous scalar value.\n",
        "  - Vector regression\n",
        "    - A task where the target is a set of continuous values\n",
        "  - Mini-batch or batch\n",
        "    - A small set of samples that are processed simultaneously by the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi_RreI8IynL"
      },
      "source": [
        "## 4.7 Under Fitting Over Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HunhwWXJDw4"
      },
      "source": [
        "![](./snaps/5.1.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3TtFN3nj86o"
      },
      "source": [
        "- Training cycle\n",
        "  - beginning\n",
        "    - optimization and generalization are correlated\n",
        "      - the lower the loss on training data, the lower the loss on test data.\n",
        "    - underfit\n",
        "  - after some iterations\n",
        "    - generalization stops improving\n",
        "    - validation metrics stall and then begin to degrade \n",
        "    - the model is starting to overfit\n",
        "      - beginning to learn patterns that are specific to the training\n",
        "- Overfitting occur when \n",
        "  - data is noisy\n",
        "  - involves uncertainty\n",
        "  - includes rare features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- if little data is available, then your validation and test sets may contain too few samples to be statistically representative of the data at hand"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.8 K-Fold Hold Out Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](./snaps/5.7.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ITERATED K-FOLD VALIDATION WITH SHUFFLING\n",
        "- apply K-fold validation multiple times, shuffling the data every time before splitting it K ways. \n",
        "- The final score is the average of the scores obtained at each run of K-fold validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.9  Spliting Techniques\n",
        "- Data representativeness\n",
        "    - reshuffle data before split\n",
        "- The arrow of time\n",
        "    - for predictions use test set made up of samples from very past\n",
        "- Redundancy in your data\n",
        "    - training and testing data should be disjoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.10 Data Preprocessing\n",
        "- vectorization\n",
        "  - turning data into tensors of floating point\n",
        "- normalization\n",
        "  - data\n",
        "    - large values / heterogeneous\n",
        "      - can trigger large gradient updates\n",
        "        - prevent the network from converging\n",
        "    - Take small values\n",
        "      - Typically, most values should be in the 0–1 range.\n",
        "    - Be homogenous\n",
        "      - all features should take values in roughly the same range.\n",
        "  - solution\n",
        "    - Normalize each feature independently to have a mean of 0.\n",
        "    - Normalize each feature independently to have a standard deviation of 1. \n",
        "- handling missing values\n",
        "  - input missing values as 0\n",
        "  - artificially generate training samples with missing entries\n",
        "    - copy some training samples several times, and drop some of the features that you expect are likely to be missing in the test dat\n",
        "- feature extraction\n",
        "    - process of using your own knowledge about the data and about the machine learning algorithm at hand to make the algorithm work better by applying hardcoded transformations to the data before it goes into the model\n",
        "    - new algorithms don't need feature engineering but can be used for two cases\n",
        "        - Good features still allow you to solve problems more elegantly while using fewer resources\n",
        "        - Good features let you solve a problem with far less data. The ability of deep learning models to learn features on their own relies on having lots of training data available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.11 Mitigate Over Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.12 Reduce Model Size\n",
        "- model that is too small will not overfit\n",
        "- to mitigate overfitting reduce the size of the model\n",
        "- limited memorization resources won’t memorize training data\n",
        "- smaller model starts overfitting later than the reference model its performance degrades more slowly once it starts overfitting.\n",
        "- bigger model starts overfitting almost immediately overfits much more severely.\n",
        "- The more capacity the model has, the more quickly it can model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large difference between the training and validation loss).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.13 Weight Regularization\n",
        "-  Simpler models are less likely to overfit than complex ones.\n",
        "- A simple model \n",
        "  - a model where the distribution of parameter values has less entropy\n",
        "- weight regularization\n",
        "  - put constraints on the complexity of a model which makes the distribution of weight values more regular.\n",
        "  - it’s done by adding to the loss function of the model a cost associated with having large weights\n",
        "- Cost\n",
        "  - L1 regularization\n",
        "    - The cost added is proportional to the absolute value of the weight coefficients\n",
        "  - L2 regularization\n",
        "    - The cost added is proportional to the square of the value of the weight coefficients\n",
        "    - L2 regularization is also called weight decay in the context of neural networks.\n",
        "- small deep learning models\n",
        "  - regularizers\n",
        "- large deep learning models\n",
        "  - dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.14 Drop out\n",
        "- dropping out a number of output features of the layer during training\n",
        "-  The dropout rate is the fraction of the features that are zeroed out\n",
        "- maximize generalization and prevent overfitting\n",
        "  - Get more training data, or better training data.\n",
        "  - Develop better features.\n",
        "  - Reduce the capacity of the model.\n",
        "  - Add weight regularization (for smaller models).\n",
        "  - Add dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.15 Universal Workflow\n",
        "\n",
        "- The universal workflow of machine learning\n",
        "  - Universal blueprint that we can use to attack and solve any machine learning problem\n",
        "    - problem defination\n",
        "    - evaluation\n",
        "    - feature engineering\n",
        "    - fighting overfitting\n",
        "- Defining the problem and assembling a dataset\n",
        "  - What will our input data be?\n",
        "  - What are we trying to predict?\n",
        "  - What type of problem we are facing?\n",
        "  - Is it binary classification?\n",
        "  - Multiclass classification?\n",
        "  - Scalr regression?\n",
        "  - Vector regression?\n",
        "  - Multiclass, multilabel classification, clustring, generation or reinforcement learning\n",
        "- Chosing a measure of success\n",
        "  - To control something we need to be able to observe it. To achieve success, we must define what we mean by success accuracy? Precision and recall? Customer-retention rate? Our metric for success will guide the choice of a loss function: what or model will optimize. It should directly align with our higher-level goals, such as the success of our business.\n",
        "- Deciding on an evaluation protocol\n",
        "  - Once we know what we are aiming for, we must esteblish how we shall measure our current progress. We have previously reviewed three common evaluation protocols:\n",
        "    - Maintaing a hold-out validation set: The way to go when plenty of data\n",
        "    - Doing K-fold cross-validation: The right choice when you have too few samples for hold-out validation to be reliable\n",
        "    - Doing iterated K-fold validation: For performing highly accurate model evaluation when little data is available\n",
        "  - for evalution just picking one of these will work, in most cases the first will work well enough\n",
        "- Preparing our data\n",
        "  - We should format our data in a way that can be fed into a machine learning model here, we shall assume a deep neural network\n",
        "    - As we saw previously our data should be formatted as tensors\n",
        "    - The values taken by these tensors should usually be scaled to small values\n",
        "    - If different features take values in different ranges then the data should be normalized\n",
        "    - We may want to do some feature engineering, especially for small-data problems\n",
        "  - Once our tensors of input data and target data are ready, we can begin to rain model\n",
        "- Developing a model that does better than baseline\n",
        "\n",
        "  - Before starting the model defination and training we must define a baseline for success criterion If we can not create a model after many successive tries, means that the hypothesis we built at step are false and we need to move back to step for gathering input data which can predict the output\n",
        "  - following table can help us chose a last -layer activation and loss function for a few common problem types\n",
        "\n",
        "  | Problem Type                            | Last layer activation | Loss function              |\n",
        "  | --------------------------------------- | --------------------- | -------------------------- |\n",
        "  | Binary classification                   | sigmoid               | binary_crossentropy        |\n",
        "  | Multiclass, single-label classification | softmax               | categorical_crossentropy   |\n",
        "  | Multiclass, multi-label classification  | sigmoid               | binary_crossentropy        |\n",
        "  | regression to arbitrary values          | None                  | mse                        |\n",
        "  | Regression to values betweeb 0 and 1    | sigmoid               | mse or binary_crossentropy |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Scaling up: developing a model that overfits\n",
        "    - Once we have built a model that satisfies the baseline criterion,  now we need to make it powerful enough that optimizes accuracy wuthout compromising the generalization. Remember the Generalization lies between the underfitting and overfitting. The easy way to get to it by just reaching the border towards overfitting. Once our model starts overfitting we know how to cope up with overfitting\n",
        "    - To make model overfit\n",
        "        - Add layers\n",
        "        - Make the layers bigger\n",
        "        - Train for more apochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Regularizing model and tuning hyperparameters\n",
        "    - This step will take the modt time: we shall repeatedly modify our model, train it, evaluate on our validation data, modify it again and repeat until the model is as good as it can get. These are somethings to try\n",
        "        - Add dropout\n",
        "        - Try different architectures: add or remove layers\n",
        "        - Add L1 and/or L2 regularization\n",
        "        - Try different hyperparameters to find the optimal configuration\n",
        "        - Optionally, iterate on feature engineering: add new features, or remove features that don't seem to be informative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.16 Summary"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "5_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
