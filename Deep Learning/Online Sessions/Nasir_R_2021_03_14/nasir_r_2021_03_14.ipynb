{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Nasir Hussain - 2021/03/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 The universal workflow of machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topics to cover\n",
    "  - Steps for framing a machine learning problem\n",
    "  - Steps for developing a working model\n",
    "  - Steps for deploying your model in production and maintaining it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In real life problems\n",
    "  - don’t start from a dataset\n",
    "  - start from a problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parts of universal workflow\n",
    "  1. Define the task\n",
    "     - Understand the problem domain and the business logic underlying what the customer asked for\n",
    "     - Collect a dataset, understand what the data represents, and choose how you will measure success on the task\n",
    "  2. Develop a model\n",
    "     - Prepare your data so that it can be processed by a machine learning model\n",
    "     - select a model evaluation protocol and a simple baseline to beat\n",
    "     - train a first model that has generalization power and that can overfit\n",
    "     - then regularize and tune your model until you achieve the best possible generalization performance\n",
    "  3. Deploy the model\n",
    "     - Present your work to stakeholders\n",
    "     - ship the model to a web server, a mobile app, a web page, or an embedded device, monitor the model’s performance in the wild, and start collecting the data you’ll need to build the next-generation model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Define the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a deep understanding of the context of what being done\n",
    "  - Why is your customer trying to solve this particular problem?\n",
    "  - What value will they derive from the solution\n",
    "  - how will your model be used\n",
    "  - how will it fit into your customer’s business processes?\n",
    "  - What kind of data is available, or could be collected?\n",
    "  - What kind of machine learning task can be mapped to the business problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps to define the task\n",
    "    - Frame the problem\n",
    "    - Collect a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Frame the problem\n",
    "\n",
    "- questions for framing the problem\n",
    "  - What will your input data be?\n",
    "  - What are you trying to predict?\n",
    "  - What type of machine learning task are you facing?\n",
    "    - binary classification\n",
    "    - Multiclass classification\n",
    "    - Scalar regression\n",
    "    - Vector regression\n",
    "    - Multiclass, multilabel classification\n",
    "    - Image segmentation\n",
    "    - Ranking\n",
    "    - clustering\n",
    "    - generation\n",
    "    - reinforcement learning\n",
    "  - What do existing solutions look like?\n",
    "  - Are there particular constraints you will need to deal with?\n",
    "- Results from above questions\n",
    "    - what your inputs will be\n",
    "    - what your targets will be,\n",
    "    - what broad type of machine learning task the problem maps to.\n",
    "- hypotheses to make\n",
    "    - your targets can be predicted given your inputs\n",
    "    - the data that’s available is sufficiently informative to learn the relationship between inputs and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Collect a dataset\n",
    "-  ability to generalize comes from the properties of the data\n",
    "    - the number of data points\n",
    "    - the reliability of labels\n",
    "    - the quality of features\n",
    "\n",
    "#### INVESTING IN DATA ANNOTATION INFRASTRUCTURE\n",
    "- options to annotate data\n",
    "    - annotate the data yourself\n",
    "    - use a crowdsourcing platform like Mechanical Turk to collect labels\n",
    "        - inexpensive and to scale well\n",
    "        - annotations may end up being quite noisy\n",
    "    - use the services of a specialized data-labeling company\n",
    "        - can potentially save you time and money\n",
    "        - it takes away control\n",
    "- constraints while annotation\n",
    "    - Do the data labelers need to be subject matter experts, or could anyone annotate the data? \n",
    "    - If annotating the data requires specialized knowledge\n",
    "        - can you train people to do it?\n",
    "        - how can you get access to relevant experts?\n",
    "    - Do you, yourself, understand the way experts come up with the annotations? \n",
    "        - If you don’t, you will have to treat your dataset as a black box, and you won’t be able to perform manual feature engineering—this isn’t critical, but it can be limiting.\n",
    "\n",
    "#### BEWARE OF NON-REPRESENTATIVE DATA\n",
    "-  models can only make sense of inputs that are similar to what they’ve seen before.\n",
    "- data used for training should be representative of the production data\n",
    "- not possible to train on production data\n",
    "    - find training and production data differ\n",
    "    - actively correct for these differences.\n",
    "- Concept drift \n",
    "    - it occurs when the properties of the production data change over time, causing model accuracy to gradually decay\n",
    "    - to deal we need constant\n",
    "        - data collection\n",
    "        - annotation\n",
    "        - model retraining\n",
    "\n",
    "##### The problem of sampling bias\n",
    "- Sampling bias occurs when your data collection process interacts with what you are trying to predict, resulting in biased measurements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Understand your data\n",
    "-  explore and visualize your data to gain insights about what makes it predictive\n",
    "    - If data includes images or natural language text\n",
    "        - take a look at a few samples directly.\n",
    "    - If data contains numerical features, \n",
    "        - plot the histogram of feature values to get a feel for \n",
    "            - the range of values taken\n",
    "            - the frequency of different values\n",
    "    - If data includes location information\n",
    "        - plot it on a map\n",
    "        - Do any clear patterns emerge?\n",
    "    - Are some samples missing values for some features? \n",
    "        - deal with this when you prepare the data\n",
    "    - If task is a classification problem\n",
    "        - print the number of instances of each class in your data.\n",
    "        - Are the classes roughly equally represented? \n",
    "            - If not, you will need to account for this imbalance.\n",
    "    - Check for target leaking\n",
    "        - the presence of features in your data that provide information about the targets and which may not be available in production.\n",
    "        - is every feature in data something that will be available in the same form in production?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 Choose a measure of success\n",
    "- success?\n",
    "    - Accuracy\n",
    "    - Precision and recall\n",
    "    - Customer retention rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Develop a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Prepare the data\n",
    "- vectorization\n",
    "- normalization\n",
    "- handling missing values\n",
    "\n",
    "#### VECTORIZATION\n",
    "- All inputs and targets in a neural network must typically be tensors of floating-point data\n",
    "- data need to process must first turn into tensors, a step called data vectorization.\n",
    "#### VALUE NORMALIZATION\n",
    "- Data should be\n",
    "    - Take small values—Typically, most values should be in the 0–1 range.\n",
    "    - Be homogenous—All features should take values in roughly the same range.\n",
    "- stricter normalization practice\n",
    "    - Normalize each feature independently to have a mean of 0.\n",
    "    - Normalize each feature independently to have a standard deviation of 1.\n",
    "        ```\n",
    "        x -= x.mean(axis=0)\n",
    "        x /= x.std(axis=0)\n",
    "        ```\n",
    "#### HANDLING MISSING VALUES\n",
    "- If the feature is categorical\n",
    "    - create a new category that means “the value is missing.”\n",
    "    - The model will automatically learn what this implies with respect to the targets.\n",
    "- If the feature is numerical, \n",
    "    - avoid inputting an arbitrary value like \"0\"\n",
    "    - replace the missing value with the average or median value for the feature in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Choose an evaluation protocol\n",
    "- evaluation protocols\n",
    "    - Maintaining a holdout validation set\n",
    "        - This is the way to go when you have plenty of data.\n",
    "    - Doing K-fold cross-validation\n",
    "        - This is the right choice when you have too few samples for holdout validation to be reliable\n",
    "    - Doing iterated K-fold validation\n",
    "        - This is for performing highly accurate model evaluation when little data is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Beat a baseline\n",
    "\n",
    "- Feature engineering\n",
    "  - Filter out uninformative features (feature selection) and use your knowledge of the problem to develop new features that are likely to be useful.\n",
    "- Selecting the correct architecture priors\n",
    "  - What type of model architecture will you use?\n",
    "    - A densely connected network\n",
    "    - a convnet\n",
    "    - a recurrent neural network\n",
    "    - a Transformer?\n",
    "    - Is deep learning even a good approach for the task, or should you use something else?\n",
    "- Selecting a good-enough training configuration\n",
    "  - What loss function should you use?\n",
    "  - What batch size and learning rate?\n",
    "\n",
    "#### Picking the right loss function\n",
    "| Problem type                            | Last-layer activation | Loss function            |\n",
    "| --------------------------------------- | --------------------- | ------------------------ |\n",
    "| Binary classification                   | sigmoid               | binary_crossentropy      |\n",
    "| Multiclass, single-label classification | softmax               | categorical_crossentropy |\n",
    "| Multiclass, multilabel classification   | sigmoid               | binary_crossentropy      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 Scale up: Develop a model that overfits\n",
    "- Once you’ve obtained a model that has statistical power, the question?\n",
    "    - is model sufficiently powerful? \n",
    "    - Does it have enough layers and parameters to properly model the problem at hand?\n",
    "- The ideal model is one that stands right at the border between \n",
    "    - underfitting and overfitting\n",
    "    - undercapacity and overcapacity.\n",
    "- develop a model that overfits\n",
    "    1. Add layers.\n",
    "    2. Make the layers bigger.\n",
    "    3. Train for more epochs.\n",
    "- model’s performance on the validation data begins to degrade, you’ve achieved overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5 Regularize and tune your model\n",
    "- maximize generalization performance.\n",
    "    - Try different architectures; add or remove layers.\n",
    "    - Add dropout.\n",
    "    - If your model is small, add L1 or L2 regularization.\n",
    "    - Try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration.\n",
    "    - Optionally, iterate on data curation or feature engineering: collect and annotate more data, develop better features, or remove features that don’t seem to be informative.\n",
    "- developed a satisfactory model configuration\n",
    "    - train your final production model on all the available data\n",
    "    - evaluate it one last time on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Explain your work to stakeholders and set expectations\n",
    "- failure modes\n",
    "- model performance expectations\n",
    "### 6.3.2 Ship an inference model\n",
    "#### DEPLOYING A MODEL AS A REST API\n",
    "- use this approch when\n",
    "    - The application that will consume the model’s prediction will have reliable access to the internet\n",
    "    - The application does not have strict latency requirements\n",
    "    - The input data sent for inference is not highly sensitive\n",
    "#### DEPLOYING A MODEL ON A DEVICE\n",
    "- use this approach when\n",
    "    - model has strict latency constraints or needs to run in a low-connectivity environment.\n",
    "    - Your model can be made sufficiently small that it can run under the memory and power constraints of the target device.\n",
    "    - Getting the highest possible accuracy isn’t mission critical for your task. There is always a trade-off between runtime efficiency and accuracy\n",
    "    - The input data is strictly sensitive and thus shouldn’t be decryptable on a remote server.\n",
    "#### DEPLOYING A MODEL IN THE BROWSER\n",
    "- You want to offload compute to the end user, which can dramatically reduce server costs.\n",
    "- The input data needs to stay on the end user’s computer or phone.\n",
    "- Your application has strict latency constraints.\n",
    "- You need your app to keep working without connectivity, after the model has been downloaded and cached.\n",
    "#### INFERENCE MODEL OPTIMIZATION\n",
    "-  popular optimization techniques\n",
    "    - Weight pruning—\n",
    "        - Not every coefficient in a weight tensor contributes equally to the predictions. It’s possible to considerably lower the number of parameters in the layers of your model by only keeping the most significant ones. This reduces the memory and compute footprint of your model, at a small cost in performance metrics. By deciding how much pruning you want to apply, you are in control of the trade-off between size and accuracy.\n",
    "    - Weight quantization\n",
    "        - Deep learning models are trained with single-precision floating-point weights. However, it’s possible to quantize weights to 8-bit signed integers to get an inferenceonly model that’s a quarter the size but remains near the accuracy of the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 Monitor your model in the wild\n",
    "- keep monitoring its behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4 Maintain your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- The universal workflow of machine learning\n",
    "    - Define the task\n",
    "        - Frame the problem\n",
    "        - Collect a dataset\n",
    "        - Understand your data\n",
    "        - Choose a measure of success\n",
    "    - Develop a model\n",
    "        - Prepare the data\n",
    "            - vectorization\n",
    "            - normalization\n",
    "            - handling missing values\n",
    "        - Choose an evaluation protocol\n",
    "            - holdout validation set\n",
    "            - K-fold cross-validation\n",
    "            - iterated K-fold cross-validation\n",
    "        - Beat a baseline\n",
    "            - Picking the right loss function\n",
    "        - Scale up: Develop a model that overfits\n",
    "            - Add layers.\n",
    "            - Make the layers bigger.\n",
    "            - Train for more epochs.\n",
    "        - Regularize and tune your model\n",
    "            - Try different architectures; add or remove layers.\n",
    "            - Add dropout.\n",
    "            - If your model is small, add L1 or L2 regularization.\n",
    "            - Try different hyperparameters (number of units, learning rate)\n",
    "            - Optionally, feature engineering\n",
    "    - Deploy the model\n",
    "        - Explain your work to stakeholders and set expectations\n",
    "        - Ship an inference model\n",
    "            - deploying a model as a rest api\n",
    "            - deploying a model on a device\n",
    "            - deploying a model in the browser\n",
    "            - inference model optimization\n",
    "        - Monitor your model in the wild\n",
    "            - keep monitoring its behavior\n",
    "        - Maintain your model\n",
    "\n",
    "- When you take on a new machine learning project, first define the problem at hand:\n",
    "    - Understand the broader context of what you’re setting out to do—what’s the end goal and what are the constraints?\n",
    "    - Collect and annotate a dataset; make sure you understand your data in depth.\n",
    "    - Choose how you’ll measure success for your problem—what metrics will you monitor on your validation data?\n",
    "- Once you understand the problem and you have an appropriate dataset, develop a model:\n",
    "    - Prepare your data.\n",
    "    - Pick your evaluation protocol: holdout validation? K-fold validation? Which portion of the data should you use for validation?\n",
    "    - Achieve statistical power: beat a simple baseline.\n",
    "    - Scale up: develop a model that can overfit.\n",
    "    - Regularize your model and tune its hyperparameters, based on performance on the validation data. A lot of machine learning research tends to focus only on this step, but keep the big picture in mind."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
