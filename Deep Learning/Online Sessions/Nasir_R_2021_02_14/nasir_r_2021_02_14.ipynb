{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Nasir Hussain - 2021/02/14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 02 - Before we begin: the mathematical building blocks of neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor\n",
    "    # Multidimesnion numpy arrays are known as tensors\n",
    "    # container of data\n",
    "# Tensor Operations\n",
    "    # Layers building blocks\n",
    "    # gears of neural networks\n",
    "# differentiation\n",
    "    # process of finding rate of change\n",
    "# gradient descent\n",
    "    # algorithm to minimize a function by moving in direction of steepest descent\n",
    "    # allows your model to learn from its training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 A first look at a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a category in a classification problem is called a class. \n",
    "# Data points are called samples. \n",
    "# The class associated with a specific sample is called a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 00:24:51.960775: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-04 00:24:51.960805: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Listing 2.1 Loading the MNIST dataset in Keras\n",
    "\n",
    "    # grayscale images\n",
    "    # 28 * 28 pixcels\n",
    "    # 10 classes\n",
    "    # 60000 training\n",
    "    # 10000 testing\n",
    "\n",
    "# import mnist data set from keras\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 2.1 Loading the MNIST dataset in Keras\n",
    "\n",
    "# split data to training and testing data sets\n",
    "# data\n",
    "    # images\n",
    "    # labels\n",
    "# train_images and train_labels form the training set,\n",
    "# test set, test_images and test_labels\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# The images are encoded as Numpy arrays\n",
    "# labels are an array of digits, ranging from 0 to 9. \n",
    "# The images and labels have a one-to-one correspondence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images :  (60000, 28, 28)\n",
      "Shape of training images labels :  (60000,)\n",
      "Labels of training images :  [5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "print(\"Shape of training images : \",train_images.shape)\n",
    "print(\"Shape of training images labels : \",train_labels.shape)\n",
    "print(\"Labels of training images : \",train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test images :  (10000, 28, 28)\n",
      "Shape of test images labels :  (10000,)\n",
      "Labels of test images :  [7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# testing data\n",
    "print(\"Shape of test images : \",test_images.shape)\n",
    "print(\"Shape of test images labels : \",test_labels.shape)\n",
    "print(\"Labels of test images : \",test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorkFlow\n",
    "    # feed the neural network the training data\n",
    "    # learn to associate images and labels\n",
    "    # ask the network to produce predictions for test_images\n",
    "    # verify whether these predictions match the labels from test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check one of the image\n",
    "import matplotlib.pylab as plt\n",
    "def plti(im, h=8, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to plot an image.\n",
    "    \"\"\"\n",
    "    y = im.shape[0]\n",
    "    x = im.shape[1]\n",
    "    w = (y/x) * h\n",
    "    plt.figure(figsize=(w, h))\n",
    "    plt.imshow(im, interpolation=\"none\", **kwargs)\n",
    "    plt.axis('off')\n",
    "plti(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer\n",
    "    # The core building block of neural networks \n",
    "    # a data-processing module (filter).\n",
    "    # Some data goes in, and it comes out in a more use-ful form.\n",
    "    # Extract representations\n",
    "# Deep learning consists of channing together simple layers whihc forms data distillation\n",
    "# Model\n",
    "    # made of refined data filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 00:24:55.736243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-04 00:24:55.736993: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-04 00:24:55.737068: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-04 00:24:55.737134: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-04 00:24:55.738905: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-04 00:24:55.738986: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-04 00:24:55.739049: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-05-04 00:24:55.739064: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Listing 2.2 The network architecture\n",
    "\n",
    "# import model and layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define network and layers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512,activation=\"relu\"),\n",
    "    layers.Dense(10,activation=\"softmax\")\n",
    "])\n",
    "# sequential model process one layer at a time\n",
    "# Dense is type of neural network\n",
    "# input shape defines shape of input data if added to layer\n",
    "# initial number is shape of output data from a layer / nodes of layer\n",
    "# activation function \n",
    "    # transform linearity to non-linearity \n",
    "    # defines how weighted sum is transformed\n",
    "# relu is rectified linear unit it returns as it is if positive other wise zero\n",
    "# softmax predict a multinomial probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation step\n",
    "# An optimizer\n",
    "    # The mechanism through which the model will update itself based on the training data it sees, \n",
    "    # so as to improve its performance.\n",
    "# A loss function\n",
    "    # How the model will be able to measure its performance on the training data, \n",
    "    # and thus how it will be able to steer itself in the right direction.\n",
    "# Metrics to monitor during training and testing\n",
    "    # Here, we’ll only care about accuracy (the fraction of the images that were correctly classified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 2.3 The compilation step\n",
    "\n",
    "model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "# rmsprop\n",
    "    # Root Mean Squared Propagation\n",
    "    # The RMSprop optimizer restricts the oscillations in the vertical direction\n",
    "# sparse_categorical_crossentropy\n",
    "    # Use this crossentropy loss function when there are two or more label classes. \n",
    "    # We expect labels to be provided as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 2.4 Preparing the image data\n",
    "\n",
    "# We need to reshape our data as it is of shape (60000,28,28) \n",
    "# on the other hand we need to use this data as initial nodes\n",
    "# thus data is reshaped to (60000,28*28)\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "# Initally values are between interval [0,225]\n",
    "# scale them to be in interval [0,1]\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "    # Continous\n",
    "    # Discrete\n",
    "        # ordinal\n",
    "            # can be re-arranged\n",
    "        # nominal\n",
    "            # can't be re-arranged\n",
    "\n",
    "# One hot encoding can be used to transform data to catgorical format\n",
    "#       A*  A   B*  B   C   D   F\n",
    "#   A*  1   0   0   0   0   0   0\n",
    "#   A   0   1   0   0   0   0   0\n",
    "#   B*  0   0   1   0   0   0   0\n",
    "#   B   0   0   0   1   0   0   0\n",
    "#   C   0   0   0   0   1   0   0\n",
    "#   D   0   0   0   0   0   1   0\n",
    "#   F   0   0   0   0   0   0   1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_labels[0])\n",
    "# # Not as such required in latest version \n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# train_labels = to_categorical(train_labels)\n",
    "# test_labels = to_categorical(test_labels)\n",
    "# print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.2549 - accuracy: 0.9260\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.1036 - accuracy: 0.9700\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0678 - accuracy: 0.9797\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0490 - accuracy: 0.9852\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0368 - accuracy: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9fb4bdaf40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 2.5 “Fitting” the model\n",
    "# train the model, \n",
    "    # we fit the model to its training data.\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "# Accuracy and loss is over the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.3593418e-09, 1.6998004e-10, 3.3544620e-07, 1.4652241e-05,\n",
       "       4.8254040e-12, 2.3962686e-08, 5.5623082e-15, 9.9998307e-01,\n",
       "       6.2424888e-08, 1.8089728e-06], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 2.6 Using the model to make predictions\n",
    "\n",
    "import numpy as np\n",
    "# Take first 10 images from the test images\n",
    "test_digits = test_images[0:10]\n",
    "# predict the model behaviour on the slice of test images\n",
    "predictions = model.predict(test_digits)\n",
    "# check predictio of first test digit\n",
    "print(np.shape(predictions[0]))\n",
    "# Each prdiction is probability of ten classes\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find index of the higest probability whihc crosponds to the same label as index\n",
    "predictions[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999831"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# higest probability\n",
    "predictions[0][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the label from the test tabels\n",
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0663 - accuracy: 0.9817\n",
      "test_acc: 0.9817000031471252\n",
      "test_loss: 0.06630222499370575\n"
     ]
    }
   ],
   "source": [
    "# Listing 2.7 Evaluating the model on new data\n",
    "\n",
    "# compute average accuracy over the entire test set\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"test_acc: {test_acc}\")\n",
    "print(f\"test_loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gap between training accuracy and test accuracy is an example of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data representations for neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensors\n",
    "    # Multi-dimensional numpy array\n",
    "    # basic data structure for machine learning\n",
    "    # matrices are rank 2 tensors\n",
    "    # dimension is also termed as axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Scalars (rank-0 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tensor that contains only one number is called a scalar\n",
    "    # scalar tensor\n",
    "    # rank-0 tensor\n",
    "    # 0D tensor\n",
    "# float32 or float64 number is a scalar tensor\n",
    "# a scalar tensor has 0 axes\n",
    "# axes == rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  12\n",
      "axes =  0\n",
      "rank =  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(12)\n",
    "print(\"X = \",x)\n",
    "print(\"axes = \",x.ndim)\n",
    "print(\"rank = \",x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Vectors (rank-1 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of numbers is called a vector,\n",
    "    # rank-1 tensor\n",
    "    # 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [12  3  6 14  7]\n",
      "axes =  1\n",
      "rank =  1\n"
     ]
    }
   ],
   "source": [
    "x = np.array([12, 3, 6, 14, 7])\n",
    "print(\"X = \",x)\n",
    "print(\"axes = \",x.ndim)\n",
    "print(\"rank = \",x.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a vector having five entries is 5-dimensional vector. \n",
    "# A 5D vector has only one axis and has five dimensions along its axis, \n",
    "# a 5D tensor has five axes\n",
    "# Dimensionality\n",
    "    # number of entries along a specific axis\n",
    "    # the number of axes in a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Matrices (rank-2 tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of vectors is a matrix\n",
    "    # rank-2 tensor\n",
    "    # 2D tensor. \n",
    "# A matrix has two axes\n",
    "    # rows\n",
    "    # columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[ 5 78  2 34  0]\n",
      " [ 6 79  3 35  1]\n",
      " [ 7 80  4 36  2]]\n",
      "axes =  2\n",
      "rank =  2\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[5, 78, 2, 34, 0],\n",
    "              [6, 79, 3, 35, 1],\n",
    "              [7, 80, 4, 36, 2]])\n",
    "print(\"X = \",x)\n",
    "print(\"axes = \",x.ndim)\n",
    "print(\"rank = \",x.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Rank-3 and higher-rank tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack of matrices in a array\n",
    "    # rank-3 tensor\n",
    "    # 3D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[[ 5 78  2 34  0]\n",
      "  [ 6 79  3 35  1]\n",
      "  [ 7 80  4 36  2]]\n",
      "\n",
      " [[ 5 78  2 34  0]\n",
      "  [ 6 79  3 35  1]\n",
      "  [ 7 80  4 36  2]]\n",
      "\n",
      " [[ 5 78  2 34  0]\n",
      "  [ 6 79  3 35  1]\n",
      "  [ 7 80  4 36  2]]]\n",
      "axes =  3\n",
      "rank =  3\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]],\n",
    "              [[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]],\n",
    "              [[5, 78, 2, 34, 0],\n",
    "               [6, 79, 3, 35, 1],\n",
    "               [7, 80, 4, 36, 2]]])\n",
    "print(\"X = \",x)\n",
    "print(\"axes = \",x.ndim)\n",
    "print(\"rank = \",x.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packing rank-3 tensors in an array => rank-4 tensor\n",
    "# rank 5 if you process video data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Key attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of axes (rank)\n",
    "    # ndim\n",
    "# Shape\n",
    "    # dimensions the tensor has along each axis\n",
    "# Data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mnist data\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimension / axes / rank of the train images\n",
    "train_images.ndim\n",
    "# it is a 3D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape train images\n",
    "train_images.shape\n",
    "# it has 60000 matrices of 28*28 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data type images\n",
    "train_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Listing 2.8 Displaying the fourth image and its label\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "digit = train_images[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "print(train_labels[4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Manipulating tensors in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor slicing\n",
    "    # Selecting specific elements in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select image #10 to #100\n",
    "my_slice = train_images[10:100]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a little detailed way of slicing\n",
    "# : is equivalent to selecting the entire axis\n",
    "my_slice = train_images[10:100, :, :]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 28, 28)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detailed way of slicing\n",
    "my_slice = train_images[10:100, 0:28, 0:28]\n",
    "my_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can select slices between any two indices along each tensor axis.\n",
    "# 14 × 14 pixels in the bottom-right corner of all images\n",
    "my_slice = train_images[:, 14:, 14:]\n",
    "digit = my_slice[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "print(train_labels[4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# possible to use negative indices\n",
    "# 14 × 14 pixels centered in the middle\n",
    "my_slice = train_images[:, 7:-7, 7:-7]\n",
    "digit = my_slice[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "print(train_labels[4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.7 The notion of data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first axis / axis 0 in all data tensors \n",
    "    # samples axis\n",
    "    # samples dimension).\n",
    "# MNIST example\n",
    "    # “samples” are images of digits.\n",
    "# deep learning models don’t process entire dataset at once; they break the data in small batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 28, 28)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first batch of size 128\n",
    "batch = train_images[:128]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 28, 28)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second batch of size 128\n",
    "batch = train_images[128:256]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 28, 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nth batch of size 128\n",
    "n = 8\n",
    "batch = train_images[128*n:128*(n+1)]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in batch tensor the first axis (axis 0) is called the batch axis or batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.8 Real-world examples of data tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector data\n",
    "    # Rank-2 tensors\n",
    "    # shape (samples, features) , \n",
    "    # sample\n",
    "        # a vector of nu-merical attributes (“features”)\n",
    "# Timeseries data or sequence data\n",
    "    # Rank-3 tensors\n",
    "    # shape (samples, timesteps, features)\n",
    "    # sample\n",
    "        # a sequence (of length timesteps ) of feature vectors\n",
    "# Images—Rank\n",
    "    # 4 tensors\n",
    "    # shape (samples, height, width, channels)\n",
    "    # sample\n",
    "        # a 2D grid of pixels, and each pixel is represented by a vector of values (“channels”)\n",
    "# Video\n",
    "    # Rank-5 tensors\n",
    "        # shape (samples, frames, height, width, channels)\n",
    "        # sample\n",
    "            # a sequence (of length frames ) of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.9 Vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each single data point can be encoded as a vector\n",
    "# a batch of data will be encoded as a rank-2 tensor \n",
    "# first axis is the samples axis and the second axis is the features axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.10 Timeseries data or sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data conatins\n",
    "    # time \n",
    "    # the notion of sequence order\n",
    "    # stores in\n",
    "        # rank-3 tensor with an explicit time axis. \n",
    "# Each sample can be encoded as a sequence of vectors (a rank-2tensor)\n",
    "# a batch of data will be encoded as a rank-3 tensor\n",
    "# The time axis is always the second axis (axis of index 1) by convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.11 Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images\n",
    "    # three dimensions\n",
    "    # rank 3\n",
    "# Image data tensor\n",
    "    # rank 4\n",
    "    # channels-last convention\n",
    "    # channels-first convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.12 Video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videos\n",
    "    # four dimensions\n",
    "    # rank 4\n",
    "# video data tensor\n",
    "    # rank 5"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
